"""LLM interface for generating refactoring suggestions using Claude API."""

import re
import time
from dataclasses import dataclass
from typing import Optional, List
import xml.etree.ElementTree as ET

from genec.core.cluster_detector import Cluster
from genec.core.dependency_analyzer import ClassDependencies
from genec.core.cluster_context_builder import ClusterContextBuilder
from genec.llm import (
    AnthropicClientWrapper,
    LLMConfig,
    LLMRequestFailed,
    LLMServiceUnavailable,
)
from genec.utils.logging_utils import get_logger

logger = get_logger(__name__)


@dataclass
class RefactoringSuggestion:
    """Represents a refactoring suggestion generated by the LLM."""
    cluster_id: int
    proposed_class_name: str
    rationale: str
    new_class_code: str
    modified_original_code: str
    cluster: Optional[Cluster] = None


class LLMInterface:
    """Interface for interacting with Claude API to generate refactoring suggestions."""

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = 'claude-sonnet-4-20250514',
        max_tokens: int = 4000,
        temperature: float = 0.3,
        timeout: int = 120,
        use_chunking: bool = True
    ):
        """
        Initialize LLM interface.

        Args:
            api_key: Anthropic API key (uses ANTHROPIC_API_KEY env var if not provided)
            model: Claude model to use
            max_tokens: Maximum tokens in response
            temperature: Sampling temperature
            timeout: Request timeout in seconds
            use_chunking: Whether to use AST-based chunking for large classes (recommended)
        """
        self.logger = get_logger(self.__class__.__name__)
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout
        self.use_chunking = use_chunking

        # Initialize context builder for chunking
        if self.use_chunking:
            self.context_builder = ClusterContextBuilder()
            self.logger.info("AST-based chunking enabled for token optimization")
        else:
            self.context_builder = None
            self.logger.info("Using full class code in prompts (legacy mode)")

        llm_config = LLMConfig(
            model=self.model,
            timeout=self.timeout,
        )
        self.llm = AnthropicClientWrapper(api_key=api_key, config=llm_config)
        if not self.llm.enabled:
            self.logger.warning(
                "Anthropic API key not provided; LLM-based suggestions will be skipped."
            )
        self._available = self.llm.enabled

    def generate_refactoring_suggestion(
        self,
        cluster: Cluster,
        original_code: str,
        class_deps: ClassDependencies,
        max_retries: int = 3
    ) -> Optional[RefactoringSuggestion]:
        """
        Generate a refactoring suggestion for a cluster using Claude.

        Args:
            cluster: Cluster to extract
            original_code: Full source code of original class
            class_deps: Class dependencies
            max_retries: Maximum retry attempts

        Returns:
            RefactoringSuggestion or None if generation fails
        """
        self.logger.info(f"Generating refactoring suggestion for cluster {cluster.id}")

        if not self._available:
            self.logger.info("LLM client unavailable; skipping suggestion generation.")
            return None

        # Build prompt
        prompt = self._build_prompt(cluster, original_code, class_deps)

        # Call Claude API with retries
        for attempt in range(max_retries):
            try:
                response = self._call_claude(prompt)
                if response:
                    # Parse response
                    suggestion = self._parse_response(response, cluster)
                    if suggestion:
                        self.logger.info(
                            f"Successfully generated suggestion: {suggestion.proposed_class_name}"
                        )
                        return suggestion
                    else:
                        self.logger.warning(f"Failed to parse response (attempt {attempt + 1})")

            except (LLMServiceUnavailable, LLMRequestFailed) as e:
                self.logger.error(
                    f"LLM call failed (attempt {attempt + 1}): {e}"
                )

                if attempt < max_retries - 1:
                    # Exponential backoff
                    wait_time = 2 ** attempt
                    self.logger.info(f"Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
            except Exception as e:
                self.logger.error(f"Unexpected error during LLM call (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    self.logger.info(f"Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)

        self.logger.error(f"Failed to generate suggestion after {max_retries} attempts")
        return None

    def _build_prompt(
        self,
        cluster: Cluster,
        original_code: str,
        class_deps: ClassDependencies
    ) -> str:
        """
        Build structured prompt for Claude.

        Uses AST-based chunking (if enabled) to send only relevant context
        instead of the full class code, reducing token usage by 90-96%.

        Args:
            cluster: Cluster to extract
            original_code: Original class source code (used if chunking disabled)
            class_deps: Class dependencies

        Returns:
            Formatted prompt string
        """
        methods = cluster.get_methods()
        fields = cluster.get_fields()

        method_list = "\n".join([f"  - {m}" for m in methods]) if methods else "  (none)"
        field_list = "\n".join([f"  - {f}" for f in fields]) if fields else "  (none)"

        # Use chunked context if enabled, otherwise fall back to full class
        if self.use_chunking and self.context_builder:
            code_context = self.context_builder.build_context(cluster, class_deps)
            context_label = "Cluster Context"
            self.logger.debug(f"Using chunked context (~{len(code_context)} chars)")
        else:
            code_context = original_code
            context_label = "Original Class"
            self.logger.debug(f"Using full class code (~{len(original_code)} chars)")

        prompt = f"""You are a software refactoring expert. Your task is to apply the Extract Class refactoring pattern to improve code quality.

**{context_label}:**
```java
{code_context}
```

**Members to Extract:**

- Methods:
{method_list}
- Fields:
{field_list}

**Your Task:**

1. Propose a descriptive class name following Java naming conventions that clearly represents the responsibility of these extracted members.

2. Write a 2-3 sentence rationale explaining why this extraction improves the design. Cite specific design principles such as:
   - Single Responsibility Principle (SRP)
   - Separation of Concerns
   - Cohesion and Coupling
   - Information Hiding

**Output Format:**

Provide your response in the following XML format:

<class_name>ProposedClassName</class_name>

<rationale>
Your 2-3 sentence explanation here, citing design principles.
</rationale>

Please provide only the XML tags specified above."""

        return prompt

    def _call_claude(self, prompt: str) -> Optional[str]:
        """
        Call Claude API with the prompt.

        Args:
            prompt: Formatted prompt

        Returns:
            Response text or None
        """
        if not self._available:
            return None

        try:
            return self.llm.send_message(
                prompt,
                model=self.model,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
            )
        except (LLMServiceUnavailable, LLMRequestFailed):
            raise

    def _parse_response(
        self,
        response: str,
        cluster: Cluster
    ) -> Optional[RefactoringSuggestion]:
        """
        Parse Claude's response to extract structured information.

        Args:
            response: Raw response text
            cluster: Original cluster

        Returns:
            RefactoringSuggestion or None if parsing fails
        """
        try:
            # Extract class name
            class_name = self._extract_xml_tag(response, 'class_name')
            if not class_name:
                self.logger.error("Failed to extract class_name")
                return None

            # Extract rationale
            rationale = self._extract_xml_tag(response, 'rationale')
            if not rationale:
                self.logger.error("Failed to extract rationale")
                return None

            return RefactoringSuggestion(
                cluster_id=cluster.id,
                proposed_class_name=class_name.strip(),
                rationale=rationale.strip(),
                new_class_code="",
                modified_original_code="",
                cluster=cluster
            )

        except Exception as e:
            self.logger.error(f"Error parsing response: {e}")
            return None

    def _extract_xml_tag(self, text: str, tag: str) -> Optional[str]:
        """
        Extract content from XML tag.

        Args:
            text: Text containing XML tags
            tag: Tag name to extract

        Returns:
            Tag content or None
        """
        # Try regex first (more robust for malformed XML)
        pattern = f'<{tag}>(.*?)</{tag}>'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(1)

        # Fallback to XML parsing
        try:
            # Wrap in root element
            wrapped = f'<root>{text}</root>'
            root = ET.fromstring(wrapped)
            element = root.find(tag)
            if element is not None and element.text:
                return element.text
        except Exception as e:
            self.logger.debug(f"XML parsing failed: {e}")

        return None

    def _clean_code(self, code: str) -> str:
        """
        Clean code by removing markdown code blocks and extra whitespace.

        Args:
            code: Code string potentially with markdown

        Returns:
            Cleaned code
        """
        # Remove markdown code blocks
        code = re.sub(r'```java\s*', '', code)
        code = re.sub(r'```\s*', '', code)

        # Remove leading/trailing whitespace from each line
        lines = code.split('\n')
        cleaned_lines = [line.rstrip() for line in lines]

        # Remove leading empty lines
        while cleaned_lines and not cleaned_lines[0].strip():
            cleaned_lines.pop(0)

        # Remove trailing empty lines
        while cleaned_lines and not cleaned_lines[-1].strip():
            cleaned_lines.pop()

        return '\n'.join(cleaned_lines)

    def generate_batch_suggestions(
        self,
        clusters: List[Cluster],
        original_code: str,
        class_deps: ClassDependencies,
        max_suggestions: int = 5
    ) -> List[RefactoringSuggestion]:
        """
        Generate refactoring suggestions for multiple clusters.

        Args:
            clusters: List of clusters to process
            original_code: Original class code
            class_deps: Class dependencies
            max_suggestions: Maximum number of suggestions to generate

        Returns:
            List of successfully generated suggestions
        """
        if not self._available:
            self.logger.info("LLM client unavailable; returning no suggestions.")
            return []

        suggestions = []

        for i, cluster in enumerate(clusters[:max_suggestions], 1):
            self.logger.info(f"Processing cluster {i}/{min(len(clusters), max_suggestions)}")

            suggestion = self.generate_refactoring_suggestion(
                cluster, original_code, class_deps
            )

            if suggestion:
                suggestions.append(suggestion)

            # Rate limiting - small delay between requests
            if i < min(len(clusters), max_suggestions):
                time.sleep(1)

        self.logger.info(f"Generated {len(suggestions)} suggestions")

        return suggestions

    def is_available(self) -> bool:
        """Return True if the LLM client is ready for use."""
        return self._available
