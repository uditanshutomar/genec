"""LLM interface for generating refactoring suggestions using Claude API."""

import re
import time
import xml.etree.ElementTree as ET
from dataclasses import dataclass, field
from typing import Optional

from genec.core.cluster_context_builder import ClusterContextBuilder
from genec.core.cluster_detector import Cluster
from genec.core.dependency_analyzer import ClassDependencies
from genec.core.evolutionary_miner import EvolutionaryData
from genec.llm import AnthropicClientWrapper, LLMConfig, LLMRequestFailed, LLMServiceUnavailable
from genec.utils.logging_utils import get_logger

# Import JDT generator for hybrid mode
try:
    from genec.core.jdt_code_generator import CodeGenerationError, JDTCodeGenerator

    JDT_AVAILABLE = True
except ImportError:
    JDT_AVAILABLE = False

logger = get_logger(__name__)

# Few-shot example for Extract Class refactoring (2024 best practice)
EXTRACT_CLASS_EXAMPLE = """
<example>
<cluster_members>
Methods:
  - public boolean validateEmail(String email)
    Description: Checks if email format is valid using regex pattern.
  - public boolean validatePhone(String phone)
    Description: Validates phone number format.
  - public boolean validateAddress(String address)
    Description: Checks if address contains required components.
Fields:
  - EMAIL_PATTERN
  - PHONE_PATTERN
</cluster_members>

<reasoning>
Step 1: Primary Responsibility Analysis
These methods share a common responsibility: input validation. They validate different types of user input (email, phone, address) using pattern matching.

Step 2: Shared Concept/Domain
All methods belong to the "validation" domain. They share validation patterns and have no business logic beyond checking format correctness.

Step 3: Extraction Benefits
- Improves Single Responsibility Principle: Validation logic is isolated
- Enhances Reusability: Validator can be used by multiple classes
- Reduces Complexity: Original class becomes simpler and more focused

Step 4: Design Justification
Follows the Extract Class refactoring pattern to separate validation concerns. The new class will have high cohesion (all methods validate input) and low coupling (minimal dependencies on other classes).
</reasoning>

<class_name>InputValidator</class_name>

<rationale>
These methods form a cohesive validation unit that should be extracted into an InputValidator class. They share validation patterns, have a single clear responsibility (input validation), and are frequently reused together. Extracting them improves code organization by separating validation concerns from business logic.
</rationale>
</example>
"""


@dataclass
class RefactoringSuggestion:
    """Represents a refactoring suggestion generated by the LLM."""

    cluster_id: int
    proposed_class_name: str
    rationale: str
    new_class_code: str
    modified_original_code: str
    cluster: Cluster | None = None
    confidence_score: float | None = None  # NEW: 0-1 confidence rating
    reasoning: str | None = None  # NEW: Chain-of-thought reasoning
    
    # Quality tier metadata
    quality_tier: str | None = None  # "should", "could", or "potential"
    quality_score: float = 0.0
    quality_reasons: list[str] = field(default_factory=list)


class LLMInterface:
    """Interface for interacting with Claude API to generate refactoring suggestions."""

    def __init__(
        self,
        api_key: str | None = None,
        model: str = "claude-sonnet-4-20250514",
        max_tokens: int = 4000,
        temperature: float = 0.3,
        timeout: int = 180,  # 3 minutes for slow connections
        use_chunking: bool = True,
        use_hybrid_mode: bool = True,  # NEW: Use JDT for code generation
        enable_confidence_scoring: bool = True,  # NEW: Ask LLM for confidence
        enable_refinement: bool = False,  # NEW: Multi-shot refinement
        jdt_code_generator: Optional["JDTCodeGenerator"] = None,  # NEW: Injected JDT
    ):
        """
        Initialize LLM interface.

        Args:
            api_key: Anthropic API key (uses ANTHROPIC_API_KEY env var if not provided)
            model: Claude model to use
            max_tokens: Maximum tokens in response
            temperature: Sampling temperature
            timeout: Request timeout in seconds
            use_chunking: Whether to use AST-based chunking for large classes
            use_hybrid_mode: Use LLM for naming + JDT for code generation
            enable_confidence_scoring: Request confidence scores from LLM
            enable_refinement: Use multi-shot refinement loop
            jdt_code_generator: JDT generator instance (created if None and hybrid mode enabled)
        """
        self.logger = get_logger(f"GenEC.{self.__class__.__name__}")
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout
        self.use_chunking = use_chunking
        self.use_hybrid_mode = use_hybrid_mode
        self.enable_confidence_scoring = enable_confidence_scoring
        self.enable_refinement = enable_refinement

        # Initialize context builder for chunking
        if self.use_chunking:
            self.context_builder = ClusterContextBuilder()
            self.logger.info("AST-based chunking enabled for token optimization")
        else:
            self.context_builder = None
            self.logger.info("Using full class code in prompts (legacy mode)")

        # Initialize JDT generator for hybrid mode
        self.jdt_generator = jdt_code_generator
        if self.use_hybrid_mode and JDT_AVAILABLE and self.jdt_generator is None:
            try:
                self.jdt_generator = JDTCodeGenerator()
                self.logger.info("Hybrid mode enabled: LLM for naming + JDT for code generation")
            except Exception as e:
                self.logger.warning(f"JDT not available, disabling hybrid mode: {e}")
                self.use_hybrid_mode = False
        elif self.use_hybrid_mode and not JDT_AVAILABLE:
            self.logger.warning("JDT not available, disabling hybrid mode")
            self.use_hybrid_mode = False

        llm_config = LLMConfig(
            model=self.model,
            timeout=self.timeout,
        )
        self.llm = AnthropicClientWrapper(api_key=api_key, config=llm_config)
        if not self.llm.enabled:
            self.logger.warning(
                "Anthropic API key not provided; LLM-based suggestions will be skipped."
            )
        self._available = self.llm.enabled

    def generate_refactoring_suggestion(
        self,
        cluster: Cluster,
        original_code: str,
        class_deps: ClassDependencies,
        class_file: str | None = None,  # NEW: for JDT hybrid mode
        repo_path: str | None = None,  # NEW: for JDT hybrid mode
        evo_data: EvolutionaryData | None = None,
        max_retries: int = 3,
    ) -> RefactoringSuggestion | None:
        """
        Generate a refactoring suggestion for a cluster using Claude.

        Args:
            cluster: Cluster to extract
            original_code: Full source code of original class
            class_deps: Class dependencies
            class_file: Path to class file (for JDT hybrid mode)
            repo_path: Repository root (for JDT hybrid mode)
            evo_data: Evolutionary data
            max_retries: Maximum retry attempts

        Returns:
            RefactoringSuggestion or None if generation fails
        """
        self.logger.info(f"Generating refactoring suggestion for cluster {cluster.id}")

        if not self._available:
            self.logger.info("LLM client unavailable; skipping suggestion generation.")
            return None

        # Use hybrid mode if available
        if self.use_hybrid_mode and self.jdt_generator and class_file and repo_path:
            return self._generate_hybrid(
                cluster, original_code, class_deps, class_file, repo_path, evo_data
            )

        # Use refinement loop if enabled
        if self.enable_refinement:
            return self._generate_with_refinement(cluster, original_code, class_deps, evo_data)

        # Standard generation (Priority 1)
        return self._generate_standard(cluster, original_code, class_deps, evo_data, max_retries)

    def _generate_standard(
        self,
        cluster: Cluster,
        original_code: str,
        class_deps: ClassDependencies,
        evo_data: EvolutionaryData | None,
        max_retries: int,
    ) -> RefactoringSuggestion | None:
        """Standard single-shot generation (Priority 1 approach)."""
        # Build prompt
        prompt = self._build_prompt(cluster, original_code, class_deps, evo_data)

        # Call Claude API with retries
        for attempt in range(max_retries):
            try:
                response = self._call_claude(prompt)
                if response:
                    # Parse response
                    suggestion = self._parse_response(response, cluster)
                    if suggestion:
                        self.logger.info(
                            f"Successfully generated suggestion: {suggestion.proposed_class_name}"
                        )
                        return suggestion
                    else:
                        self.logger.warning(f"Failed to parse response (attempt {attempt + 1})")

            except (LLMServiceUnavailable, LLMRequestFailed) as e:
                self.logger.error(f"LLM call failed (attempt {attempt + 1}): {e}")

                if attempt < max_retries - 1:
                    # Exponential backoff
                    wait_time = 2**attempt
                    self.logger.info(f"Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
            except Exception as e:
                self.logger.error(f"Unexpected error during LLM call (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    wait_time = 2**attempt
                    self.logger.info(f"Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)

        self.logger.error(f"Failed to generate suggestion after {max_retries} attempts")
        return None

    def _generate_hybrid(
        self,
        cluster: Cluster,
        original_code: str,
        class_deps: ClassDependencies,
        class_file: str,
        repo_path: str,
        evo_data: EvolutionaryData | None,
    ) -> RefactoringSuggestion | None:
        """
        Hybrid approach: LLM for naming + JDT for code generation.
        Priority 2 implementation.
        """
        self.logger.info("Using hybrid mode: LLM for naming, JDT for code")

        # Step 1: LLM generates name + rationale only
        prompt = self._build_prompt(cluster, original_code, class_deps, evo_data)

        try:
            response = self._call_claude(prompt)

            if not response:
                return None

            # Parse LLM response (name + rationale)
            parsed = self._parse_response(response, cluster)
            if not parsed:
                return None

            class_name = parsed.proposed_class_name
            rationale = parsed.rationale
            confidence = parsed.confidence_score
            reasoning = parsed.reasoning

            self.logger.info(f"LLM suggested name: {class_name}")

            # Step 2: JDT generates code deterministically
            try:
                generated = self.jdt_generator.generate(
                    cluster=cluster,
                    new_class_name=class_name,
                    class_file=class_file,
                    repo_path=repo_path,
                    class_deps=class_deps,
                )

                self.logger.info("JDT code generation successful")

                return RefactoringSuggestion(
                    cluster_id=cluster.id,
                    proposed_class_name=class_name,
                    rationale=rationale,
                    new_class_code=generated.new_class_code,
                    modified_original_code=generated.modified_original_code,
                    cluster=cluster,
                    confidence_score=confidence,
                    reasoning=reasoning,
                )

            except CodeGenerationError as e:
                self.logger.error(f"JDT code generation failed: {e}")
                return None

        except Exception as e:
            self.logger.error(f"Hybrid generation failed: {e}")
            return None

    def _generate_with_refinement(
        self,
        cluster: Cluster,
        original_code: str,
        class_deps: ClassDependencies,
        evo_data: EvolutionaryData | None,
    ) -> RefactoringSuggestion | None:
        """
        Multi-shot refinement: generate suggestion, critique, improve.
        Priority 3 implementation.
        """
        self.logger.info("Using multi-shot refinement")

        # Round 1: Initial suggestion
        prompt_v1 = self._build_prompt(cluster, original_code, class_deps, evo_data)
        response_v1 = self._call_claude(prompt_v1)

        if not response_v1:
            return None

        suggestion_v1 = self._parse_response(response_v1, cluster)
        if not suggestion_v1:
            return None

        self.logger.info(f"Initial suggestion: {suggestion_v1.proposed_class_name}")

        # Round 2: Critique and refine
        critique_prompt = f"""Review this Extract Class refactoring suggestion:

**Proposed Class Name:** {suggestion_v1.proposed_class_name}

**Rationale:** {suggestion_v1.rationale}

**Cluster Members:**
{', '.join(cluster.get_methods()[:5])}{'...' if len(cluster.get_methods()) > 5 else ''}

**Critique:**
1. Does the class name clearly convey single responsibility?
2. Is the rationale convincing and based on design principles?
3. Are there alternative names that might be clearer?
4. Any improvements needed?

**If improvements are needed**, provide a **refined** suggestion:

<class_name>RefinedClassName</class_name>

<rationale>
Improved rationale.
</rationale>

<confidence>0-1 score</confidence>

If the original is already optimal, respond with the same suggestion and confidence 0.95+.
"""

        response_v2 = self._call_claude(critique_prompt)
        if not response_v2:
            # Fallback to v1
            return suggestion_v1

        suggestion_v2 = self._parse_response(response_v2, cluster)
        if not suggestion_v2:
            # Fallback to v1
            return suggestion_v1

        # Use v2 if confidence is higher, otherwise v1
        if suggestion_v2.confidence_score and suggestion_v1.confidence_score:
            if suggestion_v2.confidence_score > suggestion_v1.confidence_score:
                self.logger.info(
                    f"Refined suggestion: {suggestion_v2.proposed_class_name} (confidence: {suggestion_v2.confidence_score:.2f})"
                )
                return suggestion_v2

        self.logger.info(f"Keeping original suggestion: {suggestion_v1.proposed_class_name}")
        return suggestion_v1

    def _build_prompt(
        self,
        cluster: Cluster,
        original_code: str,
        class_deps: ClassDependencies,
        evo_data: EvolutionaryData | None = None,
    ) -> str:
        """
        Build structured prompt for Claude.

        Optimized for JDT-based generation: sends only signatures and fields
        to minimize token usage while still allowing for accurate naming.
        """
        methods = cluster.get_methods()
        fields = cluster.get_fields()

        # Build a lightweight representation of the cluster members
        # We don't need the full body to name the class, just signatures + javadoc summary
        member_context = []

        if methods:
            member_context.append("Methods:")
            for m in methods:
                javadoc = self._extract_javadoc_summary(m, original_code)
                if javadoc:
                    member_context.append(f"  - {m}")
                    member_context.append(f"    Description: {javadoc}")
                else:
                    member_context.append(f"  - {m}")

        if fields:
            member_context.append("Fields:")
            for f in fields:
                member_context.append(f"  - {f}")

        context_str = "\n".join(member_context)

        # Format evolutionary context
        evo_context = self._format_evolutionary_context(cluster, evo_data) if evo_data else ""

        # Enhanced prompt with 2024 best practices:
        # - Few-shot example (1-shot learning)
        # - Chain-of-thought reasoning
        # - Explicit refactoring subcategories
        # - Hallucination prevention
        prompt = f"""You are a software refactoring expert specializing in the Extract Class refactoring pattern.

**Context:** I have identified a cluster of cohesive methods and fields that should be extracted from a large class to improve code quality.

**Refactoring Type:** Extract Class
**Goal:** Create a new class with high cohesion and low coupling that encapsulates a single, well-defined responsibility.

===== FEW-SHOT EXAMPLE =====
{EXTRACT_CLASS_EXAMPLE}

===== YOUR TASK =====

**Cluster Members (Signatures & Context):**
{context_str}

{evo_context}

**Instructions:**
Before suggesting a class name, please think step-by-step using the following chain-of-thought reasoning:

1. **Primary Responsibility Analysis**: What is the main responsibility shared by these methods? What do they collectively accomplish?

2. **Shared Concept/Domain**: What domain concept or abstraction do these members represent? Are they part of a cohesive subsystem?

3. **Extraction Benefits**: How will extracting these members improve the original class? Consider:
   - Single Responsibility Principle
   - Code reusability
   - Complexity reduction
   - Testability

4. **Design Justification**: Which object-oriented principle or design pattern supports this extraction?

After your step-by-step analysis, provide:
- A descriptive Java class name (UpperCamelCase, noun describing single responsibility)
- A concise rationale (2-3 sentences) explaining why these members belong together

**Quality Guidelines:**
- Class name must be a valid Java identifier (letters, digits, underscores only)
- Class name should clearly convey the single responsibility
- Avoid generic names like "Helper" or "Util" unless truly appropriate
- Rationale should reference cohesion, coupling, or design principles

**Output Format:**
Provide your response in the following XML format:

<reasoning>
Step 1: Primary Responsibility Analysis
[Your analysis here]

Step 2: Shared Concept/Domain
[Your analysis here]

Step 3: Extraction Benefits
[Your analysis here]

Step 4: Design Justification
[Your analysis here]
</reasoning>

<class_name>ProposedClassName</class_name>

<rationale>
Your 2-3 sentence explanation here.
</rationale>

<confidence>0.85</confidence>  # Rate your confidence (0.0-1.0) in this suggestion

**Important:** Provide ONLY the XML tags specified above. Do not include code generation."""

        return prompt

    def _call_claude(self, prompt: str) -> str | None:
        """
        Call Claude API with the prompt.

        Args:
            prompt: Formatted prompt

        Returns:
            Response text or None
        """
        if not self._available:
            return None

        try:
            return self.llm.send_message(
                prompt,
                model=self.model,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
            )
        except (LLMServiceUnavailable, LLMRequestFailed):
            raise

    def _parse_response(self, response: str, cluster: Cluster) -> RefactoringSuggestion | None:
        """
        Parse Claude's response to extract structured information.

        Args:
            response: Raw response text
            cluster: Original cluster

        Returns:
            RefactoringSuggestion or None if parsing fails
        """
        try:
            # Extract chain-of-thought reasoning (optional, for logging)
            reasoning = self._extract_xml_tag(response, "reasoning")
            if reasoning:
                self.logger.debug(f"LLM Reasoning:\n{reasoning}")

            # Extract class name
            class_name = self._extract_xml_tag(response, "class_name")
            if not class_name:
                self.logger.error("Failed to extract class_name")
                return None

            # Extract rationale
            rationale = self._extract_xml_tag(response, "rationale")
            if not rationale:
                self.logger.error("Failed to extract rationale")
                return None

            # Validate suggestion (hallucination prevention)
            class_name = class_name.strip()
            if not self._validate_class_name(class_name):
                self.logger.error(f"Invalid class name: {class_name}")
                return None

            # Extract confidence score (optional, Priority 3)
            confidence_str = self._extract_xml_tag(response, "confidence")
            confidence_score = None
            if confidence_str:
                try:
                    confidence_score = float(confidence_str.strip())
                    # Clamp to [0, 1]
                    confidence_score = max(0.0, min(1.0, confidence_score))
                except ValueError:
                    self.logger.warning(f"Invalid confidence score: {confidence_str}")

            return RefactoringSuggestion(
                cluster_id=cluster.id,
                proposed_class_name=class_name,
                rationale=rationale.strip(),
                new_class_code="",
                modified_original_code="",
                cluster=cluster,
                confidence_score=confidence_score,
                reasoning=reasoning,
            )

        except Exception as e:
            self.logger.error(f"Error parsing response: {e}")
            return None

    def _extract_xml_tag(self, text: str, tag: str) -> str | None:
        """
        Extract content from XML tag.

        Args:
            text: Text containing XML tags
            tag: Tag name to extract

        Returns:
            Tag content or None
        """
        # Try regex first (more robust for malformed XML)
        pattern = f"<{tag}>(.*?)</{tag}>"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(1)

        # Fallback to XML parsing
        try:
            # Wrap in root element
            wrapped = f"<root>{text}</root>"
            root = ET.fromstring(wrapped)
            element = root.find(tag)
            if element is not None and element.text:
                return element.text
        except Exception as e:
            self.logger.debug(f"XML parsing failed: {e}")

        return None

    def _clean_code(self, code: str) -> str:
        """
        Clean code by removing markdown code blocks and extra whitespace.

        Args:
            code: Code string potentially with markdown

        Returns:
            Cleaned code
        """
        # Remove markdown code blocks
        code = re.sub(r"```java\s*", "", code)
        code = re.sub(r"```\s*", "", code)

        # Remove leading/trailing whitespace from each line
        lines = code.split("\n")
        cleaned_lines = [line.rstrip() for line in lines]

        # Remove leading empty lines
        while cleaned_lines and not cleaned_lines[0].strip():
            cleaned_lines.pop(0)

        # Remove trailing empty lines
        while cleaned_lines and not cleaned_lines[-1].strip():
            cleaned_lines.pop()

        return "\n".join(cleaned_lines)

    def generate_batch_suggestions(
        self,
        clusters: list[Cluster],
        original_code: str,
        class_deps: ClassDependencies,
        class_file: str | None = None,
        repo_path: str | None = None,
        evo_data: EvolutionaryData | None = None,
        max_suggestions: int | None = None,
        max_workers: int = 4,
    ) -> list[RefactoringSuggestion]:
        """
        Generate refactoring suggestions for multiple clusters in parallel.

        Args:
            clusters: List of clusters to process
            original_code: Original class code
            class_deps: Class dependencies
            class_file: Path to class file (for JDT hybrid mode)
            repo_path: Repository root (for JDT hybrid mode)
            evo_data: Evolutionary data
            max_suggestions: Maximum number of suggestions to generate
            max_workers: Maximum number of concurrent threads

        Returns:
            List of successfully generated suggestions
        """
        if not self._available:
            self.logger.info("LLM client unavailable; returning no suggestions.")
            return []

        suggestions = []
        clusters_to_process = clusters if max_suggestions is None else clusters[:max_suggestions]
        total = len(clusters_to_process)

        self.logger.info(
            f"Generating suggestions for {total} clusters in parallel (max_workers={max_workers})..."
        )

        import concurrent.futures

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            future_to_cluster = {
                executor.submit(
                    self.generate_refactoring_suggestion,
                    cluster,
                    original_code,
                    class_deps,
                    class_file,
                    repo_path,
                    evo_data,
                ): cluster
                for cluster in clusters_to_process
            }

            # Process results as they complete
            completed_count = 0
            for future in concurrent.futures.as_completed(future_to_cluster):
                cluster = future_to_cluster[future]
                completed_count += 1
                try:
                    suggestion = future.result()
                    if suggestion:
                        suggestions.append(suggestion)
                        self.logger.info(
                            f"[{completed_count}/{total}] Successfully generated suggestion for cluster {cluster.id}"
                        )
                    else:
                        self.logger.warning(
                            f"[{completed_count}/{total}] Failed to generate suggestion for cluster {cluster.id}"
                        )
                except Exception as e:
                    self.logger.error(
                        f"[{completed_count}/{total}] Error processing cluster {cluster.id}: {e}"
                    )

        self.logger.info(f"Generated {len(suggestions)} suggestions")

        return suggestions

    def is_available(self) -> bool:
        """Return True if the LLM client is ready for use."""
        return self._available

    def _extract_javadoc_summary(self, method_signature: str, original_code: str) -> str | None:
        """
        Extract the first sentence of the Javadoc for a given method.

        Args:
            method_signature: The signature of the method to find.
            original_code: The full source code.

        Returns:
            The first sentence of the Javadoc, or None if not found.
        """
        try:
            # Extract method name from signature
            # Signature format: "public void methodName(Args...)"
            match = re.search(r"\s+(\w+)\(", method_signature)
            if not match:
                return None
            method_name = match.group(1)

            # Find the method definition in the code
            # This is a heuristic regex; it might not be perfect but works for most standard Java formatting
            # Look for Javadoc block followed by method definition
            # (/\*\*.*?\*/)\s*(?:@\w+\s*)*.*?\s+methodName\(

            # Escape method name for regex
            safe_name = re.escape(method_name)

            # Regex to find Javadoc before the method
            # 1. Capture Javadoc group
            # 2. Allow annotations and modifiers between Javadoc and method name
            pattern = re.compile(
                r"(/\*\*.*?\*/)\s*(?:@[a-zA-Z0-9_]+\(?.*?\)?\s*)*(?:public|protected|private|static|final|synchronized|native|strictfp|\s)*[\w<>\[\]]+\s+"
                + safe_name
                + r"\(",
                re.DOTALL,
            )

            match = pattern.search(original_code)
            if match:
                javadoc_block = match.group(1)
                # Strip /** and */ and *
                content = re.sub(r"/\*\*|\*/|^\s*\*", "", javadoc_block, flags=re.MULTILINE)
                # Clean up whitespace
                content = " ".join(content.split())
                # Get first sentence (split by dot followed by space or end of string)
                first_sentence = re.split(r"\.\s", content)[0]
                if first_sentence and not first_sentence.endswith("."):
                    first_sentence += "."
                return first_sentence.strip()

        except Exception:
            pass
        return None

    def _format_evolutionary_context(self, cluster: Cluster, evo_data: EvolutionaryData) -> str:
        """
        Format evolutionary coupling data for the prompt.

        Args:
            cluster: The cluster being extracted.
            evo_data: The evolutionary data.

        Returns:
            Formatted string describing historical coupling.
        """
        if not evo_data or not evo_data.cochange_matrix:
            return ""

        methods = cluster.get_methods()
        if len(methods) < 2:
            return ""

        # Check for strong coupling pairs within the cluster
        strong_couplings = []
        method_names = [
            re.search(r"\s+(\w+)\(", m).group(1) for m in methods if re.search(r"\s+(\w+)\(", m)
        ]

        # We need to map signatures back to the simple names used in evo_data keys if necessary,
        # but evo_data usually stores pairs of (methodA, methodB).
        # Assuming evo_data keys are based on the identifiers found in git diffs.

        # Let's just look for any pairs in the cluster that have high co-change counts
        threshold = 2  # Minimum co-changes to mention

        # This is a simplification. In a real scenario, we'd need robust mapping.
        # Here we just check if any pair of methods in the cluster has a high co-change count.

        count = 0
        total_co_changes = 0

        # Iterate over all pairs in the cluster
        import itertools

        for m1, m2 in itertools.combinations(method_names, 2):
            # Check both orders
            pair1 = (m1, m2)
            pair2 = (m2, m1)

            c = evo_data.cochange_matrix.get(pair1, 0) + evo_data.cochange_matrix.get(pair2, 0)
            if c >= threshold:
                count += 1
                total_co_changes += c
                if len(strong_couplings) < 3:  # Limit to top 3 examples
                    strong_couplings.append(f"{m1} and {m2} ({c} commits)")

        if count > 0:
            context = "**Evolutionary Context:**\n"
            context += f"Analysis of git history shows that methods in this cluster frequently change together ({count} strongly coupled pairs).\n"
            context += "Examples of co-evolution:\n"
            for sc in strong_couplings:
                context += f"- {sc}\n"
            context += "This historical coupling reinforces that these methods are logically related and should be refactored together."
            return context

        return ""

    def _validate_class_name(self, class_name: str) -> bool:
        """
        Validate proposed class name to prevent hallucinations.

        Args:
            class_name: Proposed class name from LLM

        Returns:
            True if valid, False otherwise
        """
        # Check 1: Valid Java identifier
        if not re.match(r"^[A-Z][a-zA-Z0-9_]*$", class_name):
            self.logger.warning(f"Class name '{class_name}' is not a valid Java identifier")
            return False

        # Check 2: Not too short (avoid single letters)
        if len(class_name) < 3:
            self.logger.warning(f"Class name '{class_name}' is too short")
            return False

        # Check 3: Not generic/unhelpful names
        generic_names = {"Helper", "Util", "Manager", "Handler", "Service", "Data", "Info"}
        if class_name in generic_names:
            self.logger.warning(f"Class name '{class_name}' is too generic")
            return False

        # Check 4: UpperCamelCase (starts with capital)
        if not class_name[0].isupper():
            self.logger.warning(f"Class name '{class_name}' should start with uppercase")
            return False

        return True
